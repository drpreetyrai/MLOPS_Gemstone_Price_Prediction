ML Pipeline with MLflow, Airflow, DVC & Docker
This repository contains a complete end-to-end machine learning pipeline setup using MLflow, Airflow, DVC, and Docker. It includes data ingestion, transformation, training, evaluation, and deployment of models with robust logging, exception handling, and experiment tracking.

![alt text](img1.png)


![alt text](img2.png)

#  End-to-End ML Pipeline with MLOps

This project is a production-ready end-to-end machine learning pipeline with integrated MLOps practices, including **data ingestion**, **transformation**, **model training**, **evaluation**, **deployment**, and **orchestration** using **Airflow**, **MLflow**, **DVC**, and **Docker**.

---

## ğŸš€ Project Setup & Installation

### Step 1: Generate Initial Template
```bash
conda create -p venv python=3.8

```

### to activate the 

```bash
conda activate /Users/preetyrai/MLOPS_Gemstone_Price_Prediction/venv

```

### Step 1: Generate Initial Template
```bash
python template.py

```

###  Step 2: Environment Setup
Make sure to initialize the environment.

```bash
bash init_setup.sh

```

This will create a virtual environment (env/ folder).

### Step 3: Install Dependencies

Write your development dependencies in requirements_dev.txt and install them:

``` bash 
pip install -r requirements_dev.txt

```

## âš™ï¸ Core Module Development 

### Step 4: Exception Handling

Create and test your custom exception class:

``` bash 
python exception.py [path]

```

### Step 5: Logging

Write tests and run them:

``` bash 
python test.py [path]

```


## ğŸ§© ML Pipeline Components
Create the following files inside the components/ directory:

* data_ingestion.py

* data_transformation.py

* model_trainer.py

* model_evaluation.py

Run ingestion to generate the artifacts/:

``` bash 
python components/data_ingestion.py [path]


```

# ğŸ› ï¸ Pipeline Integration

## Step 7: Training Pipeline

Create and run your training pipeline:

``` bash 
python pipeline/training_pipeline.py [path]


```

### Step 8: Prediction Pipeline
Create pipeline/prediction_pipeline.py

Create app.py for inference API

Create templates/ folder for frontend

## âš—ï¸ MLflow Integration
If you face issues with MLflow, follow this:

``` bash 
python -m venv myenv
source myenv/bin/activate  # Mac/Linux
myenv\Scripts\activate     # Windows
pip install setuptools


```

Then start the MLflow UI:

``` bash 
mlflow ui


```


## ğŸŒ¬ï¸ Airflow Integration
Create airflow/dags/ folder.

Add:

 * batch_prediction.py

 * training_pipeline.py

Start Airflow with Docker:

``` bash 
docker-compose up

```

Access the Airflow UI at:

``` bash 
http://localhost:8080

```


# ğŸ³ Docker + DVC Setup

## Step 1: Dockerize Your Project
Create:

Dockerfile

docker-compose.yaml

## Step 2: DVC Integration for Airflow
Create:

dvc.yaml


# â˜ï¸ Cloud Integration (CI/CD, Storage)
## DAG Configuration:

* Configure your training pipeline DAG to pull code and data from a cloud repository.

## Batch Prediction:
* Modify the DAG to:

   * Download prediction input from cloud storage.

   * Perform batch predictions.

   * Upload results back to the cloud.


# ğŸ“ Project Structure (Suggested)


``` bash 
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ data_ingestion.py
â”‚   â”œâ”€â”€ data_transformation.py
â”‚   â”œâ”€â”€ model_trainer.py
â”‚   â””â”€â”€ model_evaluation.py
â”œâ”€â”€ pipeline/
â”‚   â”œâ”€â”€ training_pipeline.py
â”‚   â””â”€â”€ prediction_pipeline.py
â”œâ”€â”€ airflow/
â”‚   â””â”€â”€ dags/
â”‚       â”œâ”€â”€ training_pipeline.py
â”‚       â””â”€â”€ batch_prediction.py
â”œâ”€â”€ templates/
â”œâ”€â”€ app.py
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yaml
â”œâ”€â”€ dvc.yaml
â”œâ”€â”€ requirements_dev.txt
â”œâ”€â”€ init_setup.sh
â”œâ”€â”€ template.py
â””â”€â”€ README.md

```

# ğŸ§  Technologies Used

* Python

* MLflow

* Apache Airflow

* Docker

* DVC

* GitHub Actions (optional for CI)

* Cloud Storage (S3/GCS/Azure Blob)

# ğŸ“Œ Future Enhancements

Add GitHub Actions for CI/CD

Implement model monitoring & drift detection

Integrate with cloud-based feature stores

Add unit and integration tests using pytest

# ğŸ™Œ Acknowledgements

Thanks to open-source tools and contributors who make scalable MLOps a reality!




